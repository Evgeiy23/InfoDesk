{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG система\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas faiss-cpu nltk pymorphy3 sentence-transformers transformers flask pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import difflib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy3\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "try:\n",
        "    _ = stopwords.words('russian')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# загружаем данные\n",
        "DATA_DIR = '.../data' # укажите путь к вашей папке data\n",
        "PREFERRED = 'lx.xlsx'\n",
        "\n",
        "xlsx_path = None\n",
        "preferred_path = os.path.join(DATA_DIR, PREFERRED)\n",
        "if os.path.isfile(preferred_path):\n",
        "    xlsx_path = preferred_path\n",
        "else:\n",
        "    cands = sorted(glob.glob(os.path.join(DATA_DIR, '*.xlsx')))\n",
        "    if cands:\n",
        "        xlsx_path = cands[0]\n",
        "\n",
        "assert xlsx_path is not None, 'Не найден ни один .xlsx в data/'\n",
        "\n",
        "print('Используем файл:', xlsx_path)\n",
        "\n",
        "df = pd.read_excel(xlsx_path)\n",
        "assert {'question','content','category'}.issubset(df.columns), 'Ожидаются столбцы question, content, category'\n",
        "\n",
        "questions = df['question'].fillna('').tolist()\n",
        "contents = df['content'].fillna('').tolist()\n",
        "categories = df['category'].fillna('прочее').tolist()\n",
        "\n",
        "# криакие названия с расшифровкой \n",
        "abbreviations = {\n",
        "    'лк': 'личный кабинет',\n",
        "    'БиР': 'Беременность и роды',\n",
        "    'зп': 'заработная плата',\n",
        "    'НДФЛ': 'Налог на доходы физических лиц',\n",
        "    'СТД': 'срочный трудовой договор',\n",
        "    'ТК': 'трудовой договор',\n",
        "    'АО': 'авансовый отчет',\n",
        "    'SLA': 'сроки',\n",
        "    'ЭЦП': 'электронная цифровая подпись',\n",
        "    'КР': 'кадровый резерв',\n",
        "}\n",
        "\n",
        "# подготовка к нормализации\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "\n",
        "def collect_vocabulary(texts):\n",
        "    vocab = set()\n",
        "    for text in texts:\n",
        "        for word in re.findall(r'\\b\\w+\\b', str(text).lower()):\n",
        "            parsed = morph.parse(word)[0]\n",
        "            vocab.add(parsed.normal_form)\n",
        "    return vocab\n",
        "\n",
        "vocabulary = collect_vocabulary(questions + contents)\n",
        "\n",
        "# предобработка текста \n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    text = str(text).lower()\n",
        "    for abbr, desc in abbreviations.items():\n",
        "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', desc, text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    corrected = []\n",
        "    for word in text.split():\n",
        "        if word not in vocabulary:\n",
        "            match = difflib.get_close_matches(word, vocabulary, n=1, cutoff=0.8)\n",
        "            if match:\n",
        "                word = match[0]\n",
        "        normal = morph.parse(word)[0].normal_form\n",
        "        if normal not in stop_words:\n",
        "            corrected.append(normal)\n",
        "    return ' '.join(set(corrected))\n",
        "\n",
        "processed_questions = [preprocess_text(q) for q in questions]\n",
        "\n",
        "# полготовка llm\n",
        "embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embeddings = embedder.encode(processed_questions, convert_to_numpy=True)\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# генератор\n",
        "generator = pipeline('text2text-generation', model='sberbank-ai/rugpt3small_based_on_gpt2')\n",
        "\n",
        "print('Готово: загружены данные, построены эмбеддинги и индекс.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional, List\n",
        "\n",
        "def rag(query: str, top_k: int = 2, category_filter: Optional[str] = None, distance_threshold: float = 1.0) -> str:\n",
        "    # преваритиельная обработка\n",
        "    processed_query = preprocess_text(query)\n",
        "\n",
        "    # фильтр категорий \n",
        "    if category_filter and category_filter != 'Все категории':\n",
        "        filtered_indices = [i for i, c in enumerate(categories) if c == category_filter]\n",
        "        if not filtered_indices:\n",
        "            return 'Нет данных для указанной категории. Соединяю с оператором.'\n",
        "        filtered_embeddings = embeddings[filtered_indices]\n",
        "        filtered_contents = [contents[i] for i in filtered_indices]\n",
        "    else:\n",
        "        filtered_embeddings = embeddings\n",
        "        filtered_contents = contents\n",
        "\n",
        "    # поиск\n",
        "    q_emb = embedder.encode([processed_query], convert_to_numpy=True)\n",
        "    temp_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    temp_index.add(filtered_embeddings)\n",
        "    distances, idxs = temp_index.search(q_emb, min(top_k, len(filtered_embeddings)))\n",
        "\n",
        "    if len(distances[0]) == 0 or distances[0][0] > distance_threshold:\n",
        "        return 'Извините, я не нашел подходящий ответ. Соединяю с оператором.'\n",
        "\n",
        "    retrieved = [filtered_contents[i] for i in idxs[0]]\n",
        "    context = ' '.join(retrieved)\n",
        "    prompt = f\"Контекст: {context}\\nВопрос: {query}\\nОтвет:\"\n",
        "\n",
        "    out = generator(prompt, max_new_tokens=150, num_return_sequences=1)\n",
        "    return out[0]['generated_text']\n",
        "\n",
        "print('Функция rag() готова.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример запроса (-ов)\n",
        "while True:\n",
        "  query = input()\n",
        "  if query != 'q':\n",
        "    print(f'{query}: {rag(query)}')\n",
        "  else:\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import difflib\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy3\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Настройка ngrok ---\n",
        "NGROK_AUTH_TOKEN = \"Token\" # https://ngrok.com/\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# --- NLP подготовка ---\n",
        "try:\n",
        "    _ = stopwords.words('russian')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "DATA_DIR = '.../data' # укажите путь к вашей папке data\n",
        "PREFERRED = 'lx.xlsx'\n",
        "\n",
        "xlsx_path = os.path.join(DATA_DIR, PREFERRED)\n",
        "if not os.path.isfile(xlsx_path):\n",
        "    raise Exception(\"Файл не найден\")\n",
        "\n",
        "df = pd.read_excel(xlsx_path)\n",
        "questions = df['question'].fillna('').tolist()\n",
        "contents = df['content'].fillna('').tolist()\n",
        "categories = df['category'].fillna('прочее').tolist()\n",
        "\n",
        "abbreviations = {\n",
        "    'лк': 'личный кабинет',\n",
        "    'БиР': 'Беременность и роды',\n",
        "    'зп': 'заработная плата',\n",
        "    'НДФЛ': 'Налог на доходы физических лиц',\n",
        "    'СТД': 'срочный трудовой договор',\n",
        "    'ТК': 'трудовой договор',\n",
        "    'АО': 'авансовый отчет',\n",
        "    'SLA': 'сроки',\n",
        "    'ЭЦП': 'электронная цифровая подпись',\n",
        "    'КР': 'кадровый резерв',\n",
        "}\n",
        "\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "def collect_vocabulary(texts):\n",
        "    vocab = set()\n",
        "    for text in texts:\n",
        "        for word in re.findall(r'\\b\\w+\\b', str(text).lower()):\n",
        "            parsed = morph.parse(word)[0]\n",
        "            vocab.add(parsed.normal_form)\n",
        "    return vocab\n",
        "\n",
        "vocabulary = collect_vocabulary(questions + contents)\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    text = str(text).lower()\n",
        "    for abbr, desc in abbreviations.items():\n",
        "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', desc, text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    corrected = []\n",
        "    for word in text.split():\n",
        "        if word not in vocabulary:\n",
        "            match = difflib.get_close_matches(word, vocabulary, n=1, cutoff=0.8)\n",
        "            if match:\n",
        "                word = match[0]\n",
        "        normal = morph.parse(word)[0].normal_form\n",
        "        if normal not in stop_words:\n",
        "            corrected.append(normal)\n",
        "    return ' '.join(set(corrected))\n",
        "\n",
        "processed_questions = [preprocess_text(q) for q in questions]\n",
        "\n",
        "# --- FAISS индекс ---\n",
        "embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embeddings = embedder.encode(processed_questions, convert_to_numpy=True)\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# --- GPT генератор (Seq2Seq модель) ---\n",
        "# Модель работает с text2text-generation\n",
        "generator = pipeline(\"text-generation\", model=\"ai-forever/rugpt3small_based_on_gpt2\")\n",
        "\n",
        "print('RAG готов к работе!')\n",
        "\n",
        "# --- Flask API ---\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask():\n",
        "    data = request.json\n",
        "    question = data.get('question', '')\n",
        "\n",
        "    # Поиск ближайшего документа\n",
        "    proc_q = preprocess_text(question)\n",
        "    q_emb = embedder.encode([proc_q], convert_to_numpy=True)\n",
        "    D, I = index.search(q_emb, k=1)\n",
        "    answer = contents[I[0][0]]\n",
        "\n",
        "    # Проверка порога похожести\n",
        "    threshold = 0.5\n",
        "    if D[0][0] > threshold:\n",
        "        answer = \"Перевожу на оператора\"\n",
        "    else:\n",
        "        answer = contents[I[0][0]]\n",
        "\n",
        "    # Генерация ответа \n",
        "    gpt_answer = generator(answer, max_new_tokens=200)[0]['generated_text']\n",
        "    return jsonify({'answer': gpt_answer})\n",
        "\n",
        "    # Поиск ответа без генерации\n",
        "    # return jsonify({'answer': answer})\n",
        "\n",
        "# --- Запуск ngrok ---\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Публичный URL для PyQt:\", public_url)\n",
        "\n",
        "# --- Запуск Flask ---\n",
        "app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
